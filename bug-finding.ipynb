{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annotated-types==0.7.0 (from -r requirements.txt (line 1))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting appnope==0.1.4 (from -r requirements.txt (line 2))\n",
      "  Downloading appnope-0.1.4-py2.py3-none-any.whl.metadata (908 bytes)\n",
      "Requirement already satisfied: asttokens==2.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.4.1)\n",
      "Collecting blis==1.0.1 (from -r requirements.txt (line 4))\n",
      "  Downloading blis-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting cachetools==5.5.0 (from -r requirements.txt (line 5))\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting catalogue==2.0.10 (from -r requirements.txt (line 6))\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: certifi==2024.8.30 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2024.8.30)\n",
      "Collecting charset-normalizer==3.4.0 (from -r requirements.txt (line 8))\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting click==8.1.7 (from -r requirements.txt (line 9))\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpathlib==0.20.0 (from -r requirements.txt (line 10))\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Collecting confection==0.1.5 (from -r requirements.txt (line 12))\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cymem==2.0.8 (from -r requirements.txt (line 13))\n",
      "  Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting debugpy==1.8.7 (from -r requirements.txt (line 14))\n",
      "  Downloading debugpy-1.8.7-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: executing==2.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (2.1.0)\n",
      "Collecting frozendict==2.4.6 (from -r requirements.txt (line 17))\n",
      "  Downloading frozendict-2.4.6-py312-none-any.whl.metadata (23 kB)\n",
      "Collecting future==1.0.0 (from -r requirements.txt (line 18))\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting html5lib-modern==1.2 (from -r requirements.txt (line 19))\n",
      "  Downloading html5lib_modern-1.2-py2.py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: idna==3.10 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (3.10)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (6.29.5)\n",
      "Collecting ipython==8.28.0 (from -r requirements.txt (line 22))\n",
      "  Downloading ipython-8.28.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jedi==0.19.1 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (0.19.1)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 24)) (3.1.4)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 26)) (5.7.2)\n",
      "Collecting langcodes==3.4.1 (from -r requirements.txt (line 27))\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language_data==1.2.0 (from -r requirements.txt (line 28))\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting LinkHeader==0.4.3 (from -r requirements.txt (line 29))\n",
      "  Downloading LinkHeader-0.4.3.tar.gz (4.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml==5.3.0 (from -r requirements.txt (line 30))\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting marisa-trie==1.2.1 (from -r requirements.txt (line 31))\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 32))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe==3.0.2 (from -r requirements.txt (line 33))\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 34)) (0.1.7)\n",
      "Collecting mdurl==0.1.2 (from -r requirements.txt (line 35))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting murmurhash==1.0.10 (from -r requirements.txt (line 36))\n",
      "  Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting MyCapytain==3.0.2 (from -r requirements.txt (line 37))\n",
      "  Downloading MyCapytain-3.0.2.tar.gz (58 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio==1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 38)) (1.6.0)\n",
      "Collecting numpy==2.0.2 (from -r requirements.txt (line 39))\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging==24.1 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 40)) (24.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 41)) (2.2.3)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 42)) (0.8.4)\n",
      "Requirement already satisfied: pexpect==4.9.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 43)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 44)) (4.3.6)\n",
      "Collecting preshed==3.0.9 (from -r requirements.txt (line 45))\n",
      "  Downloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 46)) (3.0.48)\n",
      "Collecting psutil==6.1.0 (from -r requirements.txt (line 47))\n",
      "  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 48)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 49)) (0.2.3)\n",
      "Collecting pydantic==2.9.2 (from -r requirements.txt (line 50))\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydantic_core==2.23.4 (from -r requirements.txt (line 51))\n",
      "  Downloading pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 52)) (2.18.0)\n",
      "Collecting PyLD==2.0.4 (from -r requirements.txt (line 53))\n",
      "  Downloading PyLD-2.0.4-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pyparsing==3.2.0 (from -r requirements.txt (line 54))\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 55)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz==2024.2 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 56)) (2024.2)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 57)) (26.2.0)\n",
      "Collecting rdflib==7.1.0 (from -r requirements.txt (line 58))\n",
      "  Downloading rdflib-7.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting rdflib-jsonld==0.6.2 (from -r requirements.txt (line 59))\n",
      "  Downloading rdflib_jsonld-0.6.2-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests==2.32.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 60)) (2.32.3)\n",
      "Collecting rich==13.9.2 (from -r requirements.txt (line 61))\n",
      "  Downloading rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting setuptools==75.2.0 (from -r requirements.txt (line 62))\n",
      "  Using cached setuptools-75.2.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting shellingham==1.5.4 (from -r requirements.txt (line 63))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 64)) (1.16.0)\n",
      "Collecting smart-open==7.0.5 (from -r requirements.txt (line 65))\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting spacy==3.8.2 (from -r requirements.txt (line 66))\n",
      "  Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy==3.0.12 (from -r requirements.txt (line 67))\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers==1.0.5 (from -r requirements.txt (line 68))\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting srsly==2.4.8 (from -r requirements.txt (line 69))\n",
      "  Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: stack-data==0.6.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 70)) (0.6.3)\n",
      "Collecting thinc==8.3.2 (from -r requirements.txt (line 71))\n",
      "  Downloading thinc-8.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tornado==6.4.1 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 72)) (6.4.1)\n",
      "Collecting tqdm==4.66.5 (from -r requirements.txt (line 73))\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 74)) (5.14.3)\n",
      "Collecting typer==0.12.5 (from -r requirements.txt (line 75))\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing==3.7.4.3 (from -r requirements.txt (line 76))\n",
      "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing_extensions==4.12.2 (from -r requirements.txt (line 77))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: tzdata==2024.2 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 78)) (2024.2)\n",
      "Requirement already satisfied: urllib3==2.2.3 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 79)) (2.2.3)\n",
      "Collecting wasabi==1.1.3 (from -r requirements.txt (line 80))\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in /home/codespace/.local/lib/python3.12/site-packages (from -r requirements.txt (line 81)) (0.2.13)\n",
      "Collecting weasel==0.4.1 (from -r requirements.txt (line 82))\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting wrapt==1.16.0 (from -r requirements.txt (line 83))\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading appnope-0.1.4-py2.py3-none-any.whl (4.3 kB)\n",
      "Downloading blis-1.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading cymem-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "Downloading debugpy-1.8.7-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozendict-2.4.6-py312-none-any.whl (16 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading html5lib_modern-1.2-py2.py3-none-any.whl (116 kB)\n",
      "Downloading ipython-8.28.0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.5/819.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading murmurhash-1.0.10-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyLD-2.0.4-py3-none-any.whl (70 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Downloading rdflib-7.1.0-py3-none-any.whl (562 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.1/562.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rdflib_jsonld-0.6.2-py2.py3-none-any.whl (4.0 kB)\n",
      "Downloading rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Using cached setuptools-75.2.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Downloading spacy-3.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (491 kB)\n",
      "Downloading thinc-8.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Building wheels for collected packages: LinkHeader, MyCapytain, typing\n",
      "  Building wheel for LinkHeader (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for LinkHeader: filename=LinkHeader-0.4.3-py3-none-any.whl size=4787 sha256=833e55e481938ca8881eea6d800243b9f45150fd93228b8d0e9e730f20f07bdd\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/82/da/ad/be60a633eadf20eca62e58eacf6025f9f49e4f0b3e589da72f\n",
      "  Building wheel for MyCapytain (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for MyCapytain: filename=MyCapytain-3.0.2-py3-none-any.whl size=79676 sha256=93c6527dacaf4bd0387ea9fb549d8d706a371a14d87d1f1a92cc31ec0b725e7a\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/65/bc/c7/b304f1e66f9c7bcf9e9212ed7c3336168793186b265ac57a5b\n",
      "  Building wheel for typing (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=062241d7b93da3b3eeb83f7b2562189ebae4c5d5ad8abcc76974056f2d172a06\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/12/98/52/2bffe242a9a487f00886e43b8ed8dac46456702e11a0d6abef\n",
      "Successfully built LinkHeader MyCapytain typing\n",
      "Installing collected packages: LinkHeader, cymem, wrapt, wasabi, typing_extensions, typing, tqdm, spacy-loggers, spacy-legacy, shellingham, setuptools, pyparsing, psutil, numpy, murmurhash, mdurl, MarkupSafe, lxml, html5lib-modern, future, frozendict, debugpy, cloudpathlib, click, charset-normalizer, catalogue, cachetools, appnope, annotated-types, srsly, smart-open, rdflib, PyLD, pydantic_core, preshed, markdown-it-py, marisa-trie, blis, rich, rdflib-jsonld, pydantic, language_data, ipython, typer, MyCapytain, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.2.0\n",
      "    Uninstalling pyparsing-3.2.0:\n",
      "      Successfully uninstalled pyparsing-3.2.0\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 6.1.0\n",
      "    Uninstalling psutil-6.1.0:\n",
      "      Successfully uninstalled psutil-6.1.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.8.7\n",
      "    Uninstalling debugpy-1.8.7:\n",
      "      Successfully uninstalled debugpy-1.8.7\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.0\n",
      "    Uninstalling charset-normalizer-3.4.0:\n",
      "      Successfully uninstalled charset-normalizer-3.4.0\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 8.28.0\n",
      "    Uninstalling ipython-8.28.0:\n",
      "      Successfully uninstalled ipython-8.28.0\n",
      "Successfully installed LinkHeader-0.4.3 MarkupSafe-3.0.2 MyCapytain-3.0.2 PyLD-2.0.4 annotated-types-0.7.0 appnope-0.1.4 blis-1.0.1 cachetools-5.5.0 catalogue-2.0.10 charset-normalizer-3.4.0 click-8.1.7 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.8 debugpy-1.8.7 frozendict-2.4.6 future-1.0.0 html5lib-modern-1.2 ipython-8.28.0 langcodes-3.4.1 language_data-1.2.0 lxml-5.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-2.0.2 preshed-3.0.9 psutil-6.1.0 pydantic-2.9.2 pydantic_core-2.23.4 pyparsing-3.2.0 rdflib-7.1.0 rdflib-jsonld-0.6.2 rich-13.9.2 setuptools-75.2.0 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 tqdm-4.66.5 typer-0.12.5 typing-3.7.4.3 typing_extensions-4.12.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "f = Path(f\"./tlg0525.tlg001.perseus-eng2.pickle\")\n",
    "\n",
    "def tokenize(df: pd.DataFrame):\n",
    "    model = \"en_core_web_sm\"\n",
    "\n",
    "    nlp = spacy.load(model, disable=[\"ner\"])\n",
    "\n",
    "    df[\"tokens\"] = df[\"unannotated_strings\"].apply(nlp.tokenizer)\n",
    "\n",
    "    raw_texts = [t for t in df[\"unannotated_strings\"]]\n",
    "    annotated_texts = nlp.pipe(raw_texts, batch_size=100)\n",
    "\n",
    "    df[\"nlp_docs\"] = list(annotated_texts)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = tokenize(pd.read_pickle(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 1\n",
    "\n",
    "Why is the function `expected_frequency_of_collocation(df: pd.DataFrame, node: str, collocate: str, window_size: int = 1)` returning `0.0` no matter what we pass to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def expected_frequency_of_collocation(df: pd.DataFrame, node: str, collocate: str, window_size: int = 1):\n",
    "    \"\"\"\n",
    "    `node` and `collocate` should be the string representations\n",
    "    of the associated lemmata\n",
    "    \"\"\"\n",
    "\n",
    "    lemmata = [t for t in df['nlp_docs'].explode()]\n",
    "    counter = Counter(lemmata)\n",
    "    node_count = counter[node]\n",
    "    collocate_count = counter[collocate]\n",
    "\n",
    "    return (node_count * collocate_count * window_size) / len(lemmata)\n",
    "\n",
    "expected_freq_the_mainlaind = expected_frequency_of_collocation(df, \"mainland\", \"the\")\n",
    "\n",
    "expected_freq_the_mainlaind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 1 analysis and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def expected_frequency_of_collocation(df: pd.DataFrame, node: str, collocate: str, window_size: int = 1):\n",
    "    \"\"\"\n",
    "    `node` and `collocate` should be the string representations\n",
    "    of the associated lemmata\n",
    "    \"\"\"\n",
    "\n",
    "    lemmata = [t.lemma_ for t in df['nlp_docs'].explode()]\n",
    "    counter = Counter(lemmata)\n",
    "    node_count = counter[node]\n",
    "    collocate_count = counter[collocate]\n",
    "\n",
    "    return (node_count * collocate_count * window_size) / len(lemmata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2848751741271176"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_freq_the_mainlaind = expected_frequency_of_collocation(df, \"mainland\", \"the\")\n",
    "\n",
    "expected_freq_the_mainlaind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bug was that in the list comprehension `for`, the first value was missing the `.lemma_` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 2\n",
    "\n",
    "Why does the code below not give us the number of occurrences of the token \"the\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "3165   NaN\n",
       "3166   NaN\n",
       "3167   NaN\n",
       "3168   NaN\n",
       "3169   NaN\n",
       "Name: tokens, Length: 3170, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_freq_the = df['tokens'].str.find('the')\n",
    "\n",
    "observed_freq_the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 2 analysis and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         3\n",
       "1       109\n",
       "2        29\n",
       "3         5\n",
       "4        22\n",
       "       ... \n",
       "3165      7\n",
       "3166      4\n",
       "3167     21\n",
       "3168      5\n",
       "3169    137\n",
       "Name: tokens, Length: 3170, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_freq_the = df['tokens'].astype(str).str.find('the')\n",
    "\n",
    "observed_freq_the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bug was that the `str.find` wasn't taking just the string \"the\", but the whole object that is the tokenized text. That was solved adding the method `astype` in order to specify that it only needs to take it as a string object and not the whole \"token\" object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 3\n",
    "\n",
    "How is `o11` greater than `r1`, and why is our `o12` negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1: 38, r2: 311516, c1: 26932, c2: 284622, o11: 63, o12: -25, o21: 26869\n"
     ]
    }
   ],
   "source": [
    "node = \"mainland\"\n",
    "collocate = \"the\"\n",
    "\n",
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    # the right-hand side of a slice in Python is exclusive, so we add 1 to make sure\n",
    "    # we're actually getting one element to the right\n",
    "\n",
    "    chunked_lemmata = [lemmata[i - l_size:i + r_size + 1] for i in range(0, len(lemmata))]\n",
    "    \n",
    "    cooccurrences = [1 for l in chunked_lemmata if w1 in l and w2 in l]\n",
    "\n",
    "    return sum(cooccurrences)\n",
    "\n",
    "r1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == node])\n",
    "r2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != node])\n",
    "c1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == collocate])\n",
    "c2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != collocate])\n",
    "\n",
    "o11 = df[\"nlp_docs\"].apply(count_ngram_collocations, args=(node, collocate)).sum()\n",
    "o12 = r1 - o11\n",
    "o21 = c1 - o11\n",
    "\n",
    "print(f\"r1: {r1}, r2: {r2}, c1: {c1}, c2: {c2}, o11: {o11}, o12: {o12}, o21: {o21}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bug 3 analysis and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r1: 38, r2: 311516, c1: 26932, c2: 284622, o11: 23, o12: 15, o21: 26909\n"
     ]
    }
   ],
   "source": [
    "node = \"mainland\"\n",
    "collocate = \"the\"\n",
    "\n",
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    # the right-hand side of a slice in Python is exclusive, so we add 1 to make sure\n",
    "    # we're actually getting one element to the right\n",
    "\n",
    "    chunked_lemmata = [lemmata[max(i - l_size, 0): min(i + r_size + 1, len(lemmata))] for i in range(0, len(lemmata))]\n",
    "\n",
    "    \n",
    "    cooccurrences = [1 for l in chunked_lemmata if w1 == l[1] and w2 in l]\n",
    "    \n",
    "    return sum(cooccurrences)\n",
    "\n",
    "r1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == node])\n",
    "r2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != node])\n",
    "c1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == collocate])\n",
    "c2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != collocate])\n",
    "\n",
    "o11 = df[\"nlp_docs\"].apply(count_ngram_collocations, args=(node, collocate)).sum()\n",
    "o12 = r1 - o11\n",
    "o21 = c1 - o11\n",
    "\n",
    "print(f\"r1: {r1}, r2: {r2}, c1: {c1}, c2: {c2}, o11: {o11}, o12: {o12}, o21: {o21}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cooccurrences` that was used originally is taking l not as the center of the lemmata to analyse, but rather any apparition that it may have. While this approach would allow for taking in consideration things such as \"The Greek mainland\" the new version won't, since \"mainland\" isn't the center. The reason why o12 was negative was due to the reckoning of that value: `o12 = r1 - o11`. Since o11 was bigger than r1, the result will end up being a negative number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
